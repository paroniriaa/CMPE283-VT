---CRITICAL NOTE---
All commands listed here are done in a Linux(Ubuntu) machine, so try to modify some steps/elements if you are using Windows/Mac.
For A3, we will be re-using the envirnment we created in A2 (modified L1-VM kernel, created L2-VM with nested virtualization).
Therefore, if you have the envirnment ready to go, you dont need to reproduce the steps from #0 to #38, just start from #39.

0. (Optional) You can first download and install GCP CLI tools:
        Check out https://cloud.google.com/sdk/docs/install to install it. 

1. Set up free billing account using the Google Cloud Education Credit Coupon 
        Note: Much thanks to Prof. Mike Larkin and GCP Education Team, you guys are great! :D

2. Create new project named CMPE-283, and connect it with the billing account created in step 1.

3. Enable Computer Engine API, wait for its initialization to be done.

4. Configure VM Intance on GCP:
        Go to GCP -> Compute Engine -> Virtual Machines -> VM instances -> Create instance:
        Name: Instance-1
        Region: us-west1(Oregon)
        Zone: us-west1-a
        Series: N2
        Machine type: n2-standard-4 (4 vCPU, 16GB memory)
        Boot disk:
                Public Images:
                        Operating System: Ubuntu
                        Version: Ubuntu 22.04 LTS (x86/64, amd64 jammy image built on 2022-12-01, supports Shielded VM features)
                        Boot Disk Type: SSD persistent disk
                        Size: 150 (CRITICAL: 150GB is not a must have, but you do need HUGE space for building the linux kernel, so go for the MAX you can do)
        Identity and API access: Compute Engine default service account
        Access scopes: Allow default access
        Firewall: Allow HTTP/HTTPS traffic
        Note:
                Nested Virtualization: True -> this need to be done using command line
                Min Cpu Platform: "Intel Cascade Lake" -> this need to be done using command line

5. You can save (not run) the scripts for creating a GCP VM(click on equivalent command line):
        # run gcloud commands either on your machine's shell or GCP's cloud shell
        gcloud compute instances create instance-1 --project=<gcp-project-id> --zone=us-west1-a --machine-type=n2-standard-4 --network-interface=network-tier=PREMIUM --maintenance-policy=MIGRATE --provisioning-model=STANDARD --service-account=558940717781-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --tags=http-server,https-server --create-disk=auto-delete=yes,boot=yes,device-name=instance-1,image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20221201,mode=rw,size=150,type=projects/<gcp-project-id>/zones/us-west1-a/diskTypes/pd-ssd --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any --enable-nested-virtualization --min-cpu-platform="Intel Cascade Lake"

6. Run the VM generation script on step 5 and create the desired VM on GCP:
        # located A2/scripts/, modify it before you use
        bash gcp-create-vm.sh:
                gcloud auth login

                gcloud config set project <gcp-project-id>

                gcloud compute instances create instance-1 --project=<gcp-project-id> --zone=us-west1-a --machine-type=n2-standard-4 --network-interface=network-tier=PREMIUM --maintenance-policy=MIGRATE --provisioning-model=STANDARD --service-account=558940717781-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --tags=http-server,https-server --create-disk=auto-delete=yes,boot=yes,device-name=instance-1,image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20221201,mode=rw,size=150,type=projects/<gcp-project-id>/zones/us-west1-a/diskTypes/pd-ssd --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any --enable-nested-virtualization --min-cpu-platform="Intel Cascade Lake"

7. Generate public and private SSH-KEY:
        # you should run this only one time, then copy your ssh key
        # located A2/scripts/, modify it before you use
        bash gcp-gen-ssh.sh:       
                # define $USER if needed

                ssh-keygen -t rsa -f ~/.ssh/google_cloud_key -C $USER
                
                cat /home/$USER/.ssh/google_cloud_key.pub
        
        Note: If you have terminated a VM in GCP which happens to have the same external IP address to the VM that you just create
                you will run into host authenticity failure which blocks you from establishing the SSH connection between your
                machine and the host -> because GCP reused the same IP but different ECDSA key fingerprint(public key) for 
                different VM, so your known host info recorded in your .ssh/known_hosts has a mismatch (GCP should really fix it....)
        Sol: re-create your SSH KEY using the gcp-gen-ssh.sh, do step 8 to add the fresh SSH key and delete the old SSH key, then
                you should be good to go (but the price is that you will need to add all the host info again when connecting :C)
                Also, remove your known host in your machine:
                ssh-keygen -f "/home/<your-account>/.ssh/known_hosts" -R "<VM-external-IP>"

8. Save the generated SSH public key to GCP VM Metadata:
        Go to GCP -> Compute Engine -> Settings -> Metadata -> SSH Keys -> Add Item ->
                # the format should look like the following
                Username: <your name> (it will get auto generated)
                Key: ssh-sra AAAA.....XfGI0= <your name> (just copy from ssh-sra...to the end)

9. Connect to default VM instance-1 in GCP:
        # located A2/scripts/, modify it before you use 
        bash gcp-ssh-vm.sh -e  -e <vm-external-IP>
                # define $USER if needed 
                        
                while getopts e: flag; do
                        case "${flag}" in
                        e) EXTERNAL_IP=${OPTARG} ;;
                        esac
                done

                echo "Connecting to VM $USER@$EXTERNAL_IP in GCP using ssh..."

                ssh -i ~/.ssh/google_cloud_key $USER@$EXTERNAL_IP

10. Clone the Course GitHub Repo to the VM:
        #check screenshots/VM-gcp-ssh-vm.jpg
        #check screenshots/VM-info(uname-df-nested).jpg
        # Your GitHub Repo should contain the initial files that provided by Prof. Mike Larkin:
                # 283_Assignment1-F22.pdf, cmpe283-1.c, Makefile
        # your https-based GitHub Repo looks like this: https://github.com/paroniriaa/CMPE283-VT.git
        git clone <your https-based GitHub Repo>

11. Install necessary compilation tools:
        sudo apt-get update
        sudo apt-get install gcc
        sudo apt-get install make
        sudo apt-get install flex
        sudo apt-get install bison
        sudo apt-get install libssl-dev
        sudo apt-get install libelf-dev

        #This line of command may not necessarily work as $(uname -r) does not guarantee to give the 'right' version for your linux-headers(such as WSL)
        sudo apt-get install linux-headers-$(uname -r)
        
        # If failed, try to use the following to find the right version of linux header for your machine
                apt search linux-headers
                ls -l /usr/src/linux-headers-$(uname -r)

        #---CRITICAL NOTE---: 
                If you are using WSL/WSL2, you will not be able to find the
                corresponding linux header because the base WSL2 kernel does not
                allow modules to be loaded. You have to download, compile and use 
                your own kernel build. 
                For details, refer to https://unix.stackexchange.com/questions/594470/wsl-2-does-not-have-lib-modules
                Also, here's the WSL2 kernel GitHub Repo https://github.com/microsoft/WSL2-Linux-Kernel

12. (Optional) Install GitHub for ease of use when performing push/pull to the Course GitHub Repo
        type -p curl >/dev/null || sudo apt install curl -y
curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \
&& sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \
&& echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null \
&& sudo apt update \
&& sudo apt install gh -y

13. (Optional) Login to the GitHub account for avoiding multiple auth prompts when performing push/pull:
        gh auth login
        ? What account do you want to log into? GitHub.com
        ? What is your preferred protocol for Git operations? HTTPS
        ? Authenticate Git with your GitHub credentials? Yes
        ? How would you like to authenticate GitHub CLI? Login with a web browser

14. (Optional) Install GPG for GPG key export and commit signature:
        sudo apt-get install gnupg
        gpg --list-keys 
        gpg --full-generate-key 
                RSA and RSA (default)
                4096
                1y
                Y 
                <your name> 
                <your email> 
                <your password>
                <your password again>
        gpg --list-keys 
        gpg --export --armor <your email>
        gpg --export --armor --output <some file name>.gpg.pub <your email>

15. (Optional) Configure Git global properties for auto sign using GPG key when committed:
        git config --global user.name <your name>
        git config --global user.email <your email>
        gpg --list-secret-keys --keyid-format=long
                #sec   rsa4096/<your-private-key> 
        git config --global gpg.program gpg
        git config --global user.signingKey your-private-key 
        git config --global commit.gpgsign true
        git config --global --list

        # commit your code by the following
        git add -A 
        git status
        git commit -m "<your commmit message>"
        git push

        Note: 
        you might encounter errors like:
                error: gpg failed to sign the data
                fatal: failed to write commit object
        you can fix it by:
                export GPG_TTY=$(tty)
        you might want to make it auto export whenever you initialize your bash
        so you dont have to do it every time by yourself:
                modify ~/.zshrc:
                        GPG_TTY=$(tty)
                        export GPG_TTY

        # check your signed/unsigne log history easily
        git log --pretty="format:%h %G? %aN  %s"
        
        Note: 
        In case you mess up something, dont worry, heres a trick for you to revoke commit using git:
                #n is the number of commits you wanna revokes
                git reset --soft HEAD~n
                git push origin +main --force
        In case you mess up on some branch and want to force sync your local branch with the current main branch:        
                git reset --hard origin/main

16. (CRTICAL) check if the current VM has the nested virtualization functionalities:
        #check screenshots/VM-cpuinfo-0~4.jpg
        #check screenshots/VM-lscpu-0~1.jpg
        Note: This is very very important, make sure you check it, otherwise all your following works will not be valid :C
                If you strictly followed my gcp-create-vm.sh shell script, you should be fine, as we explicitly configured that
                '--enable-nested-virtualization' when creating the VM. Still, we need to double-check:
        We can check using the following command:
       
        cat /proc/cpuinfo

        It will show detail cpu info for your machine, but we only care about the field 'vmx flags', so if you find it present
        in the cpuinfo, you are good to go.

        cat /proc/cpuinfo
                processor       : 0
                vendor_id       : GenuineIntel
                cpu family      : 6
                model           : 85
                model name      : Intel(R) Xeon(R) CPU @ 2.80GHz
                stepping        : 7
                microcode       : 0x1
                cpu MHz         : 2800.296
                cache size      : 33792 KB
                physical id     : 0
                siblings        : 4
                core id         : 1
                cpu cores       : 2
                apicid          : 3
                initial apicid  : 3
                fpu             : yes
                fpu_exception   : yes
                cpuid level     : 13
                wp              : yes
                flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities
        --->    vmx flags       : vnmi invvpid flexpriority tsc_offset vtpr vapic ept vpid unrestricted_guest vapic_reg shadow_vmcs
                bugs            : spectre_v1 spectre_v2 spec_store_bypass mds swapgs taa mmio_stale_data retbleed eibrs_pbrsb
                bogomips        : 5600.59
                clflush size    : 64
                cache_alignment : 64
                address sizes   : 46 bits physical, 48 bits virtual
                power management:

        based on GCP specification, you can also confirm the nested virtualization using another command:
        grep -cw vmx /proc/cpuinfo
                8
                any output other than 0 indicated that the nested virtualization capability is enabled

        if you want, you can check the amount of disk space available on the filesystem with each file name's argument:
        
        df -h
                Filesystem      Size  Used Avail Use% Mounted on
                /dev/root       9.6G  2.2G  7.4G  23% /
                tmpfs           3.9G     0  3.9G   0% /dev/shm
                tmpfs           1.6G  928K  1.6G   1% /run
                tmpfs           5.0M     0  5.0M   0% /run/lock
                /dev/sda15      105M  5.3M  100M   5% /boot/efi
                tmpfs           795M  4.0K  795M   1% /run/user/1001


17. Note that we actually skiped the requirement for 'download and build the Linux kernel source code' during A1,
so now we need to do it for the right configuration and environment for A2 and A3
        #check screenshots/VM-clone-linux-kernel.jpg
        Note: I took CMPE 283 during the Fall 2022 semester, and Prof. Mike Larkin did not make the requirement
                (that I mentioned above) to be mandatory during A1, but yours may be different, so you need to
                double-check, otherwise you may lose points :C.
        First, go to https://github.com/torvalds/linux, fork the repo to your account, and clone it on your VM:

        cd ~
        git clone https://github.com/<your-github-account>/linux.git
        cd linux
        git status
        git remote -v

18. The forked linux kernel may not be buildable as it is not yet correctly configured for the current VM we are running,
        #check screenshots/VM-cp-kernel-config-file.jpg
        
        so we need to find the corresponding configuration file for it:
        cd ~/linux 
        uname -r
                Note: my VM's kernel release is '5.15.0-1025-gcp', yours might be different,
                mark it as you will need it for the next command
        
        cp /boot/config-5.15.0-1025-gcp ~/linux/.config
                Note: if cp cannot find it, you need to find it yourself, cd to /boot and ls to check all available config file
                and use the matched one
        ls -la
                total 1304
                drwxrwxr-x  27 junjie junjie   4096 Dec  3 14:06 .
                drwxr-x---   6 junjie junjie   4096 Dec  3 13:58 ..
                -rw-rw-r--   1 junjie junjie  20349 Dec  3 14:05 .clang-format
                -rw-rw-r--   1 junjie junjie     59 Dec  3 14:05 .cocciconfig
        --->    -rw-r--r--   1 junjie junjie 261224 Dec  3 14:06 .config
                -rw-rw-r--   1 junjie junjie    151 Dec  3 14:05 .get_maintainer.ignore
                drwxrwxr-x   8 junjie junjie   4096 Dec  3 14:05 .git
                -rw-rw-r--   1 junjie junjie     62 Dec  3 14:05 .gitattributes
                -rw-rw-r--   1 junjie junjie   2046 Dec  3 14:05 .gitignore
                -rw-rw-r--   1 junjie junjie  24982 Dec  3 14:05 .mailmap
                -rw-rw-r--   1 junjie junjie    369 Dec  3 14:05 .rustfmt.toml
                -rw-rw-r--   1 junjie junjie    496 Dec  3 14:05 COPYING
                -rw-rw-r--   1 junjie junjie 101639 Dec  3 14:05 CREDITS
                drwxrwxr-x  86 junjie junjie   4096 Dec  3 14:05 Documentation
                -rw-rw-r--   1 junjie junjie   2573 Dec  3 14:05 Kbuild
                -rw-rw-r--   1 junjie junjie    555 Dec  3 14:05 Kconfig
                drwxrwxr-x   6 junjie junjie   4096 Dec  3 14:05 LICENSES
                -rw-rw-r--   1 junjie junjie 688453 Dec  3 14:05 MAINTAINERS
                -rw-rw-r--   1 junjie junjie  70613 Dec  3 14:05 Makefile
                -rw-rw-r--   1 junjie junjie    727 Dec  3 14:05 README
                drwxrwxr-x  24 junjie junjie   4096 Dec  3 14:05 arch
                drwxrwxr-x   3 junjie junjie   4096 Dec  3 14:05 block
                drwxrwxr-x   2 junjie junjie   4096 Dec  3 14:05 certs
                drwxrwxr-x   4 junjie junjie   4096 Dec  3 14:05 crypto
                drwxrwxr-x 139 junjie junjie   4096 Dec  3 14:05 drivers
                drwxrwxr-x  83 junjie junjie   4096 Dec  3 14:05 fs
                drwxrwxr-x  31 junjie junjie   4096 Dec  3 14:05 include
                drwxrwxr-x   2 junjie junjie   4096 Dec  3 14:05 init
                drwxrwxr-x   2 junjie junjie   4096 Dec  3 14:05 io_uring
                drwxrwxr-x   2 junjie junjie   4096 Dec  3 14:05 ipc
                drwxrwxr-x  22 junjie junjie   4096 Dec  3 14:05 kernel
                drwxrwxr-x  22 junjie junjie  12288 Dec  3 14:05 lib
                drwxrwxr-x   6 junjie junjie   4096 Dec  3 14:05 mm
                drwxrwxr-x  71 junjie junjie   4096 Dec  3 14:05 net
                drwxrwxr-x   6 junjie junjie   4096 Dec  3 14:05 rust
                drwxrwxr-x  39 junjie junjie   4096 Dec  3 14:05 samples
                drwxrwxr-x  17 junjie junjie   4096 Dec  3 14:05 scripts
                drwxrwxr-x  14 junjie junjie   4096 Dec  3 14:05 security
                drwxrwxr-x  27 junjie junjie   4096 Dec  3 14:05 sound
                drwxrwxr-x  41 junjie junjie   4096 Dec  3 14:05 tools
                drwxrwxr-x   4 junjie junjie   4096 Dec  3 14:05 usr
                drwxrwxr-x   4 junjie junjie   4096 Dec  3 14:05 virt
        
        cat into it if you like, its hella long, all related to VM configuration :]

19. build the linux kernel using the .config file we copied:
        #check screenshots/VM-make-oldconfig.jpg
        Note: if we have our .config file ready under the linux directory, the command 'make oldconfig' will take advantage of it and
        only ask the questions that is not listed in it based on the release version of the linux kernel that we're going to build

        # (CRITICAL) install these essential tools before you make oldconfig, depends on your VM, you may need to install other needed tools, try to figure it out yourself :C
        sudo apt-get install build-essential
        sudo apt-get install kernel-package
        sudo apt-get install ccache 
        sudo apt-get install flex
        sudo apt-get install bison
        sudo apt-get install libssl-dev
        sudo apt-get install libelf-dev

        make oldconfig
                Note: even though we have our .config file, we still need to answer a bunch of configuration questions, but luckily the oldconfig
                will figure out the default options for us, so we just need to press 'Enter' key all the way to the end :D 

        (CRITICAL)Note: Before you build the modules, you may want to double-check your .config file to change some content to prevent make modules from failing
        
        scripts/config --disable SYSTEM_TRUSTED_KEYS
                Note: or change the value of 'CONFIG_SYSTEM_TRUSTED_KEYS' to ""
                -> CONFIG_SYSTEM_TRUSTED_KEYS = ""
        
        scripts/config --disable SYSTEM_REVOCATION_KEYS
                Note: or change the value of 'CONFIG_SYSTEM_REVOCATION_KEYS' to ""
                -> CONFIG_SYSTEM_REVOCATION_KEYS = ""


20. prepare the linux using make prepare command:
        #check screenshots/VM-make-prepare.jpg
        make prepare
                Note: if you do everything correctly, there should be no errors and make should be successfully complete; if not, try to debug!
                        (most likely is because you did not install all the needed tools)

21. build all the modules that goes into the linux kernel:          
        #check screenshots/VM-make-modules.jpg
        make -j <your-vm-cpu-total-core-number> modules
                Note: you can use more than one CPU core to build all the modules for linux kernel
                        This is a process that would take EXTREMELY long time to finish, so MAKE SURE you use all cores available,
                        otherwise you will regret...:C

22. build the linux kernel itself:        
        #check screenshots/VM-make-kernel-0.jpg
        #check screenshots/VM-make-kernel-1.jpg
        make -j <your-vm-cpu-total-core-number> 
                Note: again, you can use more than one CPU core to build the linux kernel
                and again, this is a process that would take EXTREMELY long time to finish, so MAKE SURE you use all cores available,
                otherwise you will regret...:C


23. after the modules and kernel is built, we can package them into a format that suitable for booting inside our VM
        #check screenshots/VM-install-modules.jpg
        sudo make -j <your-vm-cpu-total-core-number> INSTALL_MOD_STRIP=1 modules_install
                Note: it takes too much space to package all modules including their debug info, so we will use INSTALL_MOD_STRIP=1
                to strip all the debugging information off the modules while they are being installed, so it is more light-weight
                for our linux kernel

24. now we can finally install our fully-built linux kernel to our VM:
        #check screenshots/VM-install-kernel.jpg
        sudo make -j <your-vm-cpu-total-core-number> install
        check 

25. (CRITICAL) reboot the VM to have it boot the newly installed linux kernel:
        #check screenshots/VM-before-reboot.jpg
        sudo reboot
        
26. (CRITICAL) now connect back to the VM, use uname command to check if our newly installed linux kernel is loaded:
        #check screenshots/VM-after-reboot.jpg
        uname -a 


27. (optional)Try to compile and run the initial cmpe283-1.c code and check the boosted linux kernel output of MSR discovery(IA32_VMX_PINBASED_CTLS for now):
        #check screenshots/VM-nested-kernel-test.jpg
        
        # the initial implementation of cmpe283-1.c should have included the discovery of IA32_VMX_PINBASED_CTLS
        # if you are using my version of cmpe283-1.c, it would print the complete MSRs discovery
        # welp we just gonna compile it and run it to make sure our working environment is compatible for A2 and A3

        make clean
        make all
        # if you are using my version of Makefile (recommended)
        make test

        if everything works fine, you should be able to see something like this:
        
        # Display the kernel log
        sudo dmesg
        [  169.300446] cmpe283_1: loading out-of-tree module taints kernel.
        [  169.300480] cmpe283_1: module verification failed: signature and/or required key missing - tainting kernel
        [  169.300722] CMPE 283 Assignment 1 Module Start
        [  169.300723] Printing Pin-Based VM-Execution Controls...
        [  169.300725] Pin-Based VM-Execution Controls MSR: 0x3f00000016
        [  169.300727] 0  External-Interrupt Exiting: Can set=Yes, Can clear=Yes
        [  169.300728] 3  NMI Exiting: Can set=Yes, Can clear=Yes
        [  169.300729] 5  Virtual NMIs: Can set=Yes, Can clear=Yes
        [  169.300730] 6  Activate VMX-Preemption Timer: Can set=No, Can clear=Yes
        [  169.300731] 7  Process Posted Interrupts: Can set=No, Can clear=Yes
        [  169.300731] Printing Primary Processor-Based VM-Execution Controls...
        (continue to print...)

        If you see the info printed above, great, now we can actually start our A2/A3 (I dont know about you, but I'm half dead already...:CCC)


28. to make changes in the kvm hypervisor, we first need to navigate to 'arch' directory,
    then check both kvm.c and cpuid.c:
    
    cd ~/linux/arch/x86/kvm
    ls

    cpuid.c is what we are looking for here

    Note: this is not related to the A2/A3, but you will see directory 'vmx' and 'svm' here, 
    which corresponding to intel brand and amd brand cpu, and we can check vmx/vmx.c to see
    how intel handle the vm exit:
        vim cmx.c 
        /handle_exit
        /kvm_emulate_cpuid
            Note: you will notice that kvm_emulate_cpuid does not define here, then it must be defined somewhere else, lets find it!
        :qa
        cd ..
        grep -R kvm_emulate_cpuid *
            Note: you will see that both CPU brands pass the exit handle to cpuid.c for kvm_emulate_cpuid, so lets take a closer look.
        vim cpuid.c
            Note: there we go! function -> int kvm_emulate_cpuid(struct kvm_vcpu *cpu) is the main character that we are looking for in A2/A3,
                which handles all the kvm cpuid exit 

29. before we do anything to change the content of kvm.c and cpuid.c, let's save the original copy for future reference and comparison purpose:
        cp ~/linux/arch/x86/kvm/cpuid.c ~/CMPE283-VT/A2/source/cpuid.origin.c  
        diff ~/linux/arch/x86/kvm/cpuid.c ~/CMPE283-VT/A2/source/cpuid.origin.c  
        cp ~/linux/arch/x86/kvm/vmx/vmx.c ~/CMPE283-VT/A2/source/vmx.origin.c
        diff ~/linux/arch/x86/kvm/vmx/vmx.c ~/CMPE283-VT/A2/source/vmx.origin.c

30. now, following the requirements in the A2/A3 instructions, we will be modifying the CPUID emulation code in KVM to report back additional information
when special CPUID leaf nodes are requested:
    A2:
        %eax=0x4FFFFFFC -> Return the total number of exits (all types) in %eax
        
        %eax=0x4FFFFFFD -> Return the high 32 bits of the total time spent processing all exits in %ebx
                            Return the low 32 bits of the total time spent processing all exits in %ecx 
                            Note: %ebx and %ecx return values are measured in processor cycles, across all VCPUs
    A3:
        %eax=0x4FFFFFFE -> Return the number of exits for the exit number provided (on input) in %ecx
                            Note: This value should be returned in %eax
        
        %eax=0x4FFFFFFF -> Return the time spent processing the exit number provided (on input) in %ecx
                            Note: Return the high 32 bits of the total time spent for that exit in %ebx
                                    Return the low 32 bits of the total time spent for that exit in %ecx
        

        (CRITICAL)Note: For leaf nodes 0x4FFFFFFE and 0x4FFFFFFF, if %ecx (on input) contains a value not defined by the
                        SDM, return 0 in all %eax, %ebx, %ecx registers and return 0xFFFFFFFF in %edx. For exit types not
                        enabled in KVM, return 0s in all four registers.

                        In other words, when dealing A3:
                        -> if the value of %ecx is not in the valid range: %eax = %ebx = %ecx = 0, %edx = 0xFFFFFFFF
                        -> if the exit types not enabled in KVM -> %eax = %ebx = %ecx = %edx = 0


        Note: For the exit number provided in %ecx, we can check each of the supported number and its indicator in the textbook
                -> Combined Volume Set of Intel® 64 and IA-32 Architectures Software Developers Manuals
                -> Volume 3 (3A, 3B, 3C & 3D): System Programming Guide
                -> Appendix C VMX Basic Exit Reasons 
                -> Table C-1. Basic Exit Reasons (page 4285)

                Notice that some basic exit reason number does not exit, for example, 35, etc.
                In other words:
                -> The valid range for %ecx: 0~34, 36~37, 39~41, 43~69
                -> The invalid range for %ecx: 35, 38, 42, N>69


31. let's modify ~/linux/arch/x86/kvm/vmx/vmx.c first for A2 requirement:
    ...
    //extern global u32 variable (from cpuid.c) for recording total number of exits
    extern atomic_t total_exits_counter;
    //extern global uint64_t variable (from cpuid.c) for recording total number of cpu cycles on exits
    extern atomic64_t total_cup_cycles_counter;

    static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
    {
        // local uint64_t variables for record the beginning and the ending of processor's time stamp counter
        uint64_t begin_time_stamp_counter, end_time_stamp_counter;
        // local int variable to store the return status of vmx handler exit function
        int ret;

        // increase by 1 for every exit
        arch_atomic_inc(&total_exits_counter);
        // record the beginning of cpu's time stamp counter
        begin_time_stamp_counter = rdtsc();

        // call the corressponding exit handler to handle the exit
        ret = __vmx_handle_exit(vcpu, exit_fastpath);

        // record the ending of cpu's time stamp counter
        end_time_stamp_counter = rdtsc();
        // compute the current time stamp gap and add it to the total cpu cycle time
        arch_atomic64_add((end_time_stamp_counter - begin_time_stamp_counter), &total_cup_cycles_counter);

        ...
    }
    ...


32. then modify ~/linux/arch/x86/kvm/cpuid.c for A2 requirement:
    ...
    //volatile int counter for total_exits_counter, atomically initialize to 0
    atomic_t total_exits_counter = ATOMIC_INIT(0);
    //export the variable total_exits_counter so vmx.c can use it
    EXPORT_SYMBOL(total_exits_counter);

    //volatile int64 counter for total_cup_cycles_counter, atomically initialize to 0
    atomic64_t total_cup_cycles_counter = ATOMIC64_INIT(0);
    //export the variable total_cup_cycles_counter so vmx.c can use it
    EXPORT_SYMBOL(total_cup_cycles_counter);

    int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)
    {
        u32 eax, ebx, ecx, edx;

        if (cpuid_fault_enabled(vcpu) && !kvm_require_cpl(vcpu, 0))
            return 1;

        eax = kvm_rax_read(vcpu);
        ecx = kvm_rcx_read(vcpu);

        // check special new CPUID leaf that defined in A2
        switch(eax) {
            // case %eax = 0x4FFFFFFC
            case 0x4FFFFFFC:
                eax = arch_atomic_read(&total_exits_counter);
                //printk(KERN_INFO "### Total Exits in EAX = %u", eax);
                break;

            // case %eax = 0x4FFFFFFD
            case 0x4FFFFFFD:
                //the high 32 bits of the total time spent processing all exits store in %ebx
                ebx = (atomic64_read(&total_cup_cycles_counter) >> 32);;
                //the low 32 bits of the total time spent processing all exits store in %ecx
                ecx = (atomic64_read(&total_cup_cycles_counter) & 0xFFFFFFFF);

                //printk(KERN_INFO "### Total CPU Exit Cycle Time(hi) in EBX = %u", ebx);
                //printk(KERN_INFO "### Total CPU Exit Cycle Time(lo) in ECX = %u", ecx);
                break;

            // default case for all other %eax value
            default:
                kvm_cpuid(vcpu, &eax, &ebx, &ecx, &edx, false);
            }

                kvm_rax_write(vcpu, eax);
                ...
        }
        ...

33. go back to the top level of ~/linux to build and install the modules again, then install the kernel:
        make -j <your-vm-cpu-total-core-number> modules
        sudo make -j <your-vm-cpu-total-core-number> INSTALL_MOD_STRIP=1 modules_install && make install
        sudo lsmod | grep kvm
        sudo rmmod kvm_intel
        sudo rmmod kvm
        sudo modprobe kvm
        sudo modprobe kvm_intel
        sudo lsmod | grep kvm

34. if there is no error, then we are ready to create a inner VM inside our current VM,
but first we need to install some extra tools for KVM, then check kvm-ok, and reboot:
        sudo apt-get install cpu-checker
        sudo apt update
        sudo apt-get install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils
        sudo kvm-ok
        sudo reboot


35. now we need to authorize a vm users for the inner VM, and verify our VM functionalities:
        uname -a
        sudo adduser '<your-name>' libvirt
                Note: you can check all your libvirt usser using the following command:
                        sudo getent group | grep libvirt
        sudo adduser '<your-name>' kvm
                Note: you can check all your kvm usser using the following command:
                sudo getent group | grep kvm
        sudo virsh list --all
                Note: it should show an empty list
        sudo systemctl status libvirtd
                Note: if everything is functioning properly, the output returns an active (running) status.
                        if it is not activate, do the following command to activate it
                        sudo systemctl enable --now libvirtd


36. now, we want to install some basic GUI functionalities on our VM for inner-vm interaction:
        Note: I tried so many ways to interact with the inner-VM using CLI only but failed,
                if you figured out how to host an inner-VM inside the current VM, do let me know! :d

        Note: Assuming you are following my step and hosted your VM on GCP, we can utilize the GUI functionalities
                supported by GCP (coooool)
        sudo apt update
        sudo apt install --assume-yes wget tasksel
        wget https://dl.google.com/linux/direct/chrome-remote-desktop_current_amd64.deb
        sudo apt-get install --assume-yes ./chrome-remote-desktop_current_amd64.deb
        sudo tasksel install ubuntu-desktop
        sudo bash -c ‘echo “exec /etc/X11/Xsession /usr/bin/gnome-session” > /etc/chrome-remote-desktop-session’
                Note: if it failed, its okay! :D
        sudo apt-get install xbase-clients
        sudo apt-get install python3-psutil
        sudo apt --fix-broken install
        sudo dpkg -i chrome-remote-desktop_current_amd64.deb
        mkdir ~/.config/chrome-remote-desktop
        exit

37. to fully enable the GUI functionalities provided by GCP, we need to first stop our VM and check the visual support box:
        Go to your GCP -> Compute Instance -> Stop the VM
        give it a few moment to stop...
        Then, go to your VM -> Edit -> Machine Configuration -> Enable Display Device -> Start the VM
        give it a few moment to start...

        then log back in to our VM:
                bash gcp-ssh-vm.sh -e <your-vm-external-IP>
        
38. cool! Now lets go to your localhost machine and open up your Chrome browser:
        Go to https://remotedesktop.google.com/headless
        (CRITICAL) Log in using the same account which you use for your GCP VM
        
        Note: For the following procedure, You only need to perform once
        
        Set up via SSH -> 
        Set uo another computer-> 
        begin -> next -> authorize ->
        copy the script with title 'Debian Linux' ->
        paste it into your VM terminal -> run ->

        Then you will get the following:
                Enter a PIN of at least six digits:
                Enter the same PIN again:
                [1204/020103.286183:INFO:daemon_controller_delegate_linux.cc(98)] Created symlink /etc/systemd/system/multi-user.target.wants/chrome-remote-desktop@junjie.service → /lib/systemd/system/chrome-remote-desktop@.service.

        You can test if the remote connection is set up by:
                sudo systemctl status chrome-remote-desktop@$USER

        Mark down the 6-digits pin you enter as we will bw using that connect to our VM

        Go back to your chrome browser -> remote access -> choose can click on your registered VM instance -> 
        type the 6-digits pin -> done! :D

                Note: the chrome-based GUI display of our VM terminal has configuration as well,
                        you can click on the eight edge of your window and pull it out. I recommended
                        setting your FPS from 30(default) to 60, so it looks less laggy when we interact
                        with our inner-VM later on

36. okay! we can now use virt-manager to create our inner VM along with the GUI:
        sudo apt install virt-manager
        cd ~
        wget https://releases.ubuntu.com/jammy/ubuntu-22.04.1-desktop-amd64.iso
        sudo mv ~/ubuntu-22.04.1-desktop-amd64.iso /var/lib/libvirt/images/
        sudo virt-manager
        
        If you get everything done correctly, the virt-manager GUI should pop up on your
        chrome browser already and ready for VM configuration:

        create a new virtual machine-> forward ->
        browse the OS image ISO -> 
        choose ubuntu-22.04.1-desktop-amd64.iso ->
        choose volume -> forward ->
        Memory -> half of your VM memory ->
        CPU -> all of your VM CPUs -> forward
        create a disk image for the virtual machine -> half of your VM memory -> forward
        name -> ubuntu-vm -> finish

        try or install ubuntu ->

        on and on and on....you should be quite familiar with ubuntu OS installation aren't you ;]

        After the ubuntu OS is installed, it will perform a reboot.

        Note: depends on your KVM's configurations, you may encounter situation where the inner-vm
                keeps frozen after the reboot -> it is common, and it is okay to force shut down and then
                start the VM again. 
        
37. now we can perform our A2 exit handling testing on the newly installed inner-VM:
        but first we need essential building tools:
        sudo apt-get update
        sudo apt-get install build-essential
        sudo apt-get install cpuid

        depends on your habit, you can either go ahead and write test.c or clone you existing A2 repo
        and write test.C

        A2_test.c:
                #include <stdio.h>
                #include <sys/types.h>


                static inline void
                __cpuid(unsigned int *eax, unsigned int *ebx, unsigned int *ecx,
                unsigned int *edx)
                {
                    asm volatile("cpuid"
                    : "=a" (*eax),"=b" (*ebx),"=c" (*ecx),"=d" (*edx)
                    : "0" (*eax), "1" (*ebx), "2" (*ecx), "3" (*edx));
                }

                int
                main(int argc, char **argv)
                {
                    unsigned int eax, ebx, ecx, edx;
                    unsigned long long cycle_time;

                    eax = 0x4FFFFFFC;
                    __cpuid(&eax, &ebx, &ecx, &edx);
                    printf("CPUID(0x4FFFFFFC), Total exit counter = %u \n", eax);

                    eax = 0x4FFFFFFD;
                    __cpuid(&eax, &ebx, &ecx, &edx);
                    cycle_time = (unsigned long long) ebx << 32 | ecx;
                    printf("CPUID(0x4FFFFFFD), Total exit cycles = %llu \n", cycle_time);
                }
        Makefile:
              default: build
              build:
              	g++ -o A2-test A2-test.c
              run:
              	./A2-test

38. compile and run A2-test.c to verify the result of CPUID leaf eax = 0x4FFFFFFC and eax = 0x4FFFFFFD:

    make
    make test

    A2-test result:
        (1st run)
        ./A2-test
        CPUID(0x4FFFFFFC), Total exit counter = 6460757
        CPUID(0x4FFFFFFD), Total exit cycles = 67151925918
        ---wait few seconds betweeen runs---
        (2nd run)
        ./A2-test
        CPUID(0x4FFFFFFC), Total exit counter = 6466720
        CPUID(0x4FFFFFFD), Total exit cycles = 67174390690
        ---wait few seconds betweeen runs---
        (3rd run)
        ./A2-test
        CPUID(0x4FFFFFFC), Total exit counter = 6471035
        CPUID(0x4FFFFFFD), Total exit cycles = 67191636674
        ---wait few seconds betweeen runs---
        (4th run)
        ./A2-test
        CPUID(0x4FFFFFFC), Total exit counter = 6474505
        CPUID(0x4FFFFFFD), Total exit cycles = 67204865408
        ...
        (you can continue to run and test but I think the result pattern is obvious enough...)

        We can see that based on the CPUID leaf node we implemented on 
        %eax = 0x4FFFFFFC and %eax = 0x4FFFFFFD, our total exit counter and 
        cpu cycles will be increasing continuously as basic exits (regardless of the type)
        constantly happens in our L2-VM (inner VM) and trigged the L1-VM 
        (outer VM) to handle the exits and update the total exit counter
        and total exit cycles we implemented in cpuid.c and vmx.c

        Now we know that our VM triggers the exit handler function all the time,
        but we don't know the exact exit couter and exit cpu cycle time for 
        differnt types of exits. It is very important to get to know these numbers
        as some type of exits happens a lot more frequently than others, and some type of 
        exits take a lot of cpu cycle time, while others only take a few. Lets move 
        forward to A3 where we will be implementing the other 2 CPUID leaf node 
        %eax = 0x4FFFFFFE and %eax = 0x4FFFFFFF to find out the secrets of those
        numbers! :D 

39. We need to revisit cpuid.c and vmx.c to figure out the requirements A4 and implement
        the leaf nodes:


TBD...

